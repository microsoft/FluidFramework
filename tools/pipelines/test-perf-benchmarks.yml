# Copyright (c) Microsoft Corporation and contributors. All rights reserved.
# Licensed under the MIT License.

name: $(Build.BuildId)

trigger: none
pr: none

resources:
  pipelines:
  - pipeline: client   # Name of the pipeline resource
    source: Build - client packages
    branch: main # Default branch for manual/scheduled triggers if none is selected
    trigger:
      branches:
      - release/*
      - main
      - next
      - lts

parameters:

- name: poolBuild
  type: object
  default: Large-eastus2

- name: memoryTestPackages
  type: object
  default:
    - "@fluidframework/sequence"
    - "@fluidframework/map"
    - "@fluidframework/matrix"

- name: executionTestPackages
  type: object
  default:
    - "@fluidframework/tree"
    - "@fluid-experimental/tree"
    - "@fluidframework/merge-tree"
    - "@fluidframework/container-runtime"

# List of packages running custom benchmark tests.
- name: customBenchmarkTestPackages
  type: object
  default:
    - "@fluidframework/tree"

# Performance e2e tests
- name: endpoints
  type: object
  default:
  - endpointName: 'local'
  - endpointName: 'odsp'
    lockVariableGroupName: 'perf-odsp-lock'
    timeoutInMinutes: 360
  - endpointName: 'frs'
    lockVariableGroupName: 'perf-frs-lock'
    timeoutInMinutes: 360

variables:
  # We use 'chalk' to colorize output, which auto-detects color support in the
  # running terminal.  The log output shown in Azure DevOps job runs only has
  # basic ANSI color support though, so force that in the pipeline
  - name: FORCE_COLOR
    value: 1
    readonly: true
  - name: testWorkspace
    value: $(Pipeline.Workspace)/test
    readonly: true
  - name: testFilesPath
    value: $(Pipeline.Workspace)/test-files
    readonly: true
  - name: artifactPipeline
    value: $(resources.pipeline.client.pipelineName)
    readonly: true
  - name: artifactBuildId
    value: $(resources.pipeline.client.runID)
    readonly: true
  - name: pathToTelemetryGenerator
    value: $(Agent.TempDirectory)/telemetry-generator)
    readonly: true
  - name: pathToTelemetryGeneratorHandlers
    value: $(pathToTelemetryGenerator)/node_modules/@ff-internal/telemetry-generator/dist/handlers/
    readonly: true
  - group: prague-key-vault
  - group: ado-feeds
  - name: pipelineIdentifierForTelemetry
    value: 'PerformanceBenchmark'
    readonly: true
  # This is a test pipeline, not a build one, so we don't need to run CodeQL tasks
  - name: DisableCodeQL
    value: true

lockBehavior: sequential
stages:
  # Performance unit tests - runtime
  - stage: perf_unit_tests_runtime
    displayName: Perf unit tests - runtime
    dependsOn: []
    variables:
      - name: consolidatedTestsOutputFolder
        value: ${{ variables.testWorkspace }}/benchmarkOutput
        readonly: true
    jobs:
    - job: perf_unit_tests_runtime
      displayName: Perf unit tests - runtime
      pool: ${{ parameters.poolBuild }}
      steps:
      - template: templates/include-test-perf-benchmarks.yml
        parameters:
          # ado-feeds-dev and ado-feeds-office come from the ado-feeds variable group
          artifactBuildId: $(artifactBuildId)
          artifactPipeline: $(artifactPipeline)
          devFeedUrl: $(ado-feeds-dev)
          officeFeedUrl: $(ado-feeds-office)
          testFilesPath: $(testFilesPath)
          testWorkspace: $(testWorkspace)
          pathForTelemetryGeneratorInstall: $(pathToTelemetryGenerator)

      # Run tests for each package that has them
      - ${{ each testPackage in parameters.executionTestPackages }}:

        # Download and install package with performance tests
        - template: templates/include-test-perf-benchmarks-install-package.yml
          parameters:
            artifactBuildId: $(artifactBuildId)
            artifactPipeline: $(artifactPipeline)
            testPackageName: ${{ testPackage }}
            installPath: $(testWorkspace)

        # The "npm pack"-ed package that we install doesn't have the test files.
        # Unpack them "on top" of the package, where it was installed, so we can run its tests.
        - task: Bash@3
          displayName: Unpack test files - ${{ testPackage }}
          inputs:
            targetType: 'inline'
            script: |
              set -eu -o pipefail
              TAR_FILENAME=${{ replace(replace(replace(replace(testPackage, '@fluidframework/', '' ), '@fluid-internal/', '' ),'@fluid-', '' ), '/', '-') }}
              TAR_PATH=$(testFilesPath)/$TAR_FILENAME.test-files.tar
              cd ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
              echo "Unpacking test files for ${{ testPackage }} from file '$TAR_PATH' in '$(pwd)'"
              # Note: we could skip the last argument and have it unpack everything at once, but if we later change
              # the structure/contents of the tests tar file, the extraction could overwrite things we didn't intend to,
              # so keeping the paths to extract explicit.
              # Also, extracting is finicky with the exact format of the last argument, it needs to match how the
              # tarfile was created (e.g. './lib/test' works here but 'lib/test' does not).
              tar --extract --verbose --file $TAR_PATH ./lib/test
              tar --extract --verbose --file $TAR_PATH ./dist/test
              tar --extract --verbose --file $TAR_PATH ./src/test

        # Install package dependencies (this gets devDependencies installed so we can then run the package's tests).
        # Note: the required .npmrc file is created by the include-test-perf-benchmarks.yml template above.
        - task: CopyFiles@2
          displayName: Copy .npmrc to test package folder - ${{ testPackage }}
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}
            contents: '.npmrc'
            targetFolder: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
        - task: Npm@1
          displayName: Install dependencies - ${{ testPackage }}
          retryCountOnTaskFailure: 1
          inputs:
            command: 'install'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}

        # Run tests
        - task: Npm@1
          displayName: Run execution-time tests - ${{ testPackage }}
          inputs:
            command: 'custom'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
            customCommand: 'run test:benchmark:report'

        # Consolidate output files
        - task: CopyFiles@2
          displayName: Consolidate output files - ${{ testPackage }}
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}/node_modules/@fluid-tools/benchmark/dist/.output/
            contents: '**'
            targetFolder: ${{ variables.consolidatedTestsOutputFolder }}/${{ testPackage }}

        # Cleanup package
        - task: Bash@3
          displayName: Cleanup package - ${{ testPackage }}
          inputs:
            targetType: 'inline'
            workingDirectory: ${{ variables.testWorkspace }}/node_modules/
            script: |
              set -eu -o pipefail
              echo "Cleanup package ${{ testPackage }} from ${{ variables.testWorkspace }}/node_modules/"
              rm -rf ${{ testPackage }};

      - task: Bash@3
        displayName: Write measurements to Aria/Kusto
        inputs:
          targetType: 'inline'
          workingDirectory: $(pathToTelemetryGenerator)
          script: |
            set -eu -o pipefail
            echo "Write the following benchmark output to Aria/Kusto"
            ls -laR ${{ variables.consolidatedTestsOutputFolder }};
            npx telemetry-generator --handlerModule $(pathToTelemetryGeneratorHandlers)/executionTimeTestHandler.js --dir '${{ variables.consolidatedTestsOutputFolder }}';

      - task: Bash@3
        displayName: Send Execution Time Performance Benchmark Measurments to Azure App Insights
        inputs:
          targetType: 'inline'
          workingDirectory: $(pathToTelemetryGenerator)
          script: |
            set -eu -o pipefail
            echo "Writing performance benchmark output to Azure App Insights..."
            ls -laR ${{ variables.consolidatedTestsOutputFolder }};
            npx telemetry-generator appInsights --handlerModule $(pathToTelemetryGeneratorHandlers)/appInsightsExecutionTimeTestHandler.js --dir '${{ variables.consolidatedTestsOutputFolder }}' --connectionString '$(fluid-interal-app-insights-connection-string)';
        env:
          BUILD_ID: $(Build.BuildId)
          BRANCH_NAME: $(Build.SourceBranchName)

      - task: PublishPipelineArtifact@1
        displayName: Publish Artifact - Perf tests output - execution time
        inputs:
          targetPath: '${{ variables.consolidatedTestsOutputFolder }}'
          artifactName: 'perf-test-outputs_execution-time'
        condition: succeededOrFailed()

  - template: /tools/pipelines/templates/include-upload-stage-telemetry.yml@self
    parameters:
      stageId: perf_unit_tests_runtime
      pipelineIdentifierForTelemetry: ${{ variables.pipelineIdentifierForTelemetry }}
      testWorkspace: ${{ variables.testWorkspace }}

  # Performance unit tests - memory
  - stage: perf_unit_tests_memory
    displayName: Perf unit tests - memory
    dependsOn: []
    variables:
      - name: consolidatedTestsOutputFolder
        value: ${{ variables.testWorkspace }}/memoryTestsOutput
        readonly: true
    jobs:
    - job: perf_unit_tests_memory
      displayName: Perf unit tests - memory
      pool: ${{ parameters.poolBuild }}
      steps:
      - template: templates/include-test-perf-benchmarks.yml
        parameters:
          # ado-feeds-dev and ado-feeds-office come from the ado-feeds variable group
          artifactBuildId: $(artifactBuildId)
          artifactPipeline: $(artifactPipeline)
          devFeedUrl: $(ado-feeds-dev)
          officeFeedUrl: $(ado-feeds-office)
          testFilesPath: $(testFilesPath)
          testWorkspace: $(testWorkspace)

      # Run tests for each package that has them
      - ${{ each testPackage in parameters.memoryTestPackages }}:

        # Download and install package with performance tests
        - template: templates/include-test-perf-benchmarks-install-package.yml
          parameters:
            artifactBuildId: $(artifactBuildId)
            artifactPipeline: $(artifactPipeline)
            testPackageName: ${{ testPackage }}
            installPath: $(testWorkspace)

        # The "npm pack"-ed package that we install doesn't have the test files.
        # Unpack them "on top" of the package, where it was installed, so we can run its tests.
        - task: Bash@3
          displayName: Unpack test files - ${{ testPackage }}
          inputs:
            targetType: 'inline'
            script: |
              set -eu -o pipefail
              TAR_FILENAME=${{ replace(replace(replace(replace(testPackage, '@fluidframework/', '' ), '@fluid-internal/', '' ),'@fluid-', '' ), '/', '-') }}
              TAR_PATH=$(testFilesPath)/$TAR_FILENAME.test-files.tar
              cd ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
              echo "Unpacking test files for ${{ testPackage }} from file '$TAR_PATH' in '$(pwd)'"
              # Note: we could skip the last argument and have it unpack everything at once, but if we later change
              # the structure/contents of the tests tar file, the extraction could overwrite things we didn't intend to,
              # so keeping the paths to extract explicit.
              # Also, extracting is finicky with the exact format of the last argument, it needs to match how the
              # tarfile was created (e.g. './lib/test' works here but 'lib/test' does not).
              tar --extract --verbose --file $TAR_PATH ./lib/test
              tar --extract --verbose --file $TAR_PATH ./dist/test
              tar --extract --verbose --file $TAR_PATH ./src/test

        # Install package dependencies (this gets devDependencies installed so we can then run the package's tests)
        # Note: the required .npmrc file is created by the include-test-perf-benchmarks.yml template above.
        - task: CopyFiles@2
          displayName: Copy .npmrc to test package folder - ${{ testPackage }}
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}
            contents: '.npmrc'
            targetFolder: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
        - task: Npm@1
          displayName: Install dependencies - ${{ testPackage }}
          retryCountOnTaskFailure: 1
          inputs:
            command: 'install'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}

        # Run tests
        - task: Npm@1
          displayName: Run memory usage tests - ${{ testPackage }}
          inputs:
            command: 'custom'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
            customCommand: 'run test:memory-profiling:report'

        # Consolidate output files
        - task: CopyFiles@2
          displayName: Consolidate output files - ${{ testPackage }}
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}/.memoryTestsOutput
            contents: '**'
            targetFolder: ${{ variables.consolidatedTestsOutputFolder }}/${{ testPackage }}

        # Cleanup package
        - task: Bash@3
          displayName: Cleanup package - ${{ testPackage }}
          inputs:
            targetType: 'inline'
            workingDirectory: ${{ variables.testWorkspace }}/node_modules/
            script: |
              set -eu -o pipefail
              echo "Cleanup package ${{ testPackage }} from ${{ variables.testWorkspace }}/node_modules/"
              rm -rf ${{ testPackage }};

      - task: Bash@3
        displayName: Write measurements to Aria/Kusto
        inputs:
          targetType: 'inline'
          workingDirectory: $(pathToTelemetryGenerator)
          script: |
            set -eu -o pipefail
            echo "Write the following benchmark output to Aria/Kusto";
            ls -laR ${{ variables.consolidatedTestsOutputFolder }};
            npx telemetry-generator --handlerModule $(pathToTelemetryGeneratorHandlers)/memoryUsageTestHandler.js --dir ${{ variables.consolidatedTestsOutputFolder }};

      - task: Bash@3
        displayName: Send Memory Usage Performance Benchmark Measurments to Azure App Insights
        inputs:
          targetType: 'inline'
          workingDirectory: $(pathToTelemetryGenerator)
          script: |
            set -eu -o pipefail
            echo "Writing performance benchmark output to Azure App Insights..."
            ls -laR ${{ variables.consolidatedTestsOutputFolder }};
            npx telemetry-generator appInsights --handlerModule $(pathToTelemetryGeneratorHandlers)/appInsightsMemoryUsageTestHandler.js --dir '${{ variables.consolidatedTestsOutputFolder }}' --connectionString '$(fluid-interal-app-insights-connection-string)';
        env:
          BUILD_ID: $(Build.BuildId)
          BRANCH_NAME: $(Build.SourceBranchName)

      - task: PublishPipelineArtifact@1
        displayName: Publish Artifact - Perf tests output - memory usage
        inputs:
          targetPath: '${{ variables.consolidatedTestsOutputFolder }}'
          artifactName: 'perf-test-outputs_memory-usage'
        condition: succeededOrFailed()

  - template: /tools/pipelines/templates/include-upload-stage-telemetry.yml@self
    parameters:
      stageId: perf_unit_tests_memory
      pipelineIdentifierForTelemetry: ${{ variables.pipelineIdentifierForTelemetry }}
      testWorkspace: ${{ variables.testWorkspace }}

  # Performance unit tests - customBenchmark
  - stage: perf_unit_tests_customBenchmark
    displayName: Perf unit tests - customBenchmark
    dependsOn: []
    variables:
      - name: consolidatedTestsOutputFolder
        value: ${{ variables.testWorkspace }}/customBenchmarksOutput
        readonly: true
    jobs:
    - job: perf_unit_tests_customBenchmark
      displayName: Perf unit tests - customBenchmark
      pool: ${{ parameters.poolBuild }}
      steps:
      - template: templates/include-test-perf-benchmarks.yml
        parameters:
          # ado-feeds-dev and ado-feeds-office come from the ado-feeds variable group
          artifactBuildId: $(artifactBuildId)
          artifactPipeline: $(artifactPipeline)
          devFeedUrl: $(ado-feeds-dev)
          officeFeedUrl: $(ado-feeds-office)
          testFilesPath: $(testFilesPath)
          testWorkspace: $(testWorkspace)

      # Run tests for each package that has them
      - ${{ each testPackage in parameters.customBenchmarkTestPackages }}:

        # Download and install package with performance tests
        - template: templates/include-test-perf-benchmarks-install-package.yml
          parameters:
            artifactBuildId: $(artifactBuildId)
            artifactPipeline: $(artifactPipeline)
            testPackageName: ${{ testPackage }}
            installPath: $(testWorkspace)

        # The "npm pack"-ed package that we install doesn't have the test files.
        # Unpack them "on top" of the package, where it was installed, so we can run its tests.
        - task: Bash@3
          displayName: Unpack test files - ${{ testPackage }}
          inputs:
            targetType: 'inline'
            script: |
              set -eu -o pipefail
              TAR_FILENAME=${{ replace(replace(replace(replace(testPackage, '@fluidframework/', '' ), '@fluid-internal/', '' ),'@fluid-', '' ), '/', '-') }}
              TAR_PATH=$(testFilesPath)/$TAR_FILENAME.test-files.tar
              cd ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
              echo "Unpacking test files for ${{ testPackage }} from file '$TAR_PATH' in '$(pwd)'"
              # Note: we could skip the last argument and have it unpack everything at once, but if we later change
              # the structure/contents of the tests tar file, the extraction could overwrite things we didn't intend to,
              # so keeping the paths to extract explicit.
              # Also, extracting is finicky with the exact format of the last argument, it needs to match how the
              # tarfile was created (e.g. './lib/test' works here but 'lib/test' does not).
              tar --extract --verbose --file $TAR_PATH ./lib/test
              tar --extract --verbose --file $TAR_PATH ./dist/test
              tar --extract --verbose --file $TAR_PATH ./src/test

        # Install package dependencies (this gets devDependencies installed so we can then run the package's tests)
        # Note: the required .npmrc file is created by the include-test-perf-benchmarks.yml template above.
        - task: CopyFiles@2
          displayName: Copy .npmrc to test package folder - ${{ testPackage }}
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}
            contents: '.npmrc'
            targetFolder: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}

        - task: Npm@1
          displayName: Install dependencies - ${{ testPackage }}
          retryCountOnTaskFailure: 1
          inputs:
            command: 'install'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}

        - task: Npm@1
          displayName: Run custom data performance tests - ${{ testPackage }}
          inputs:
            command: 'custom'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}
            customCommand: 'run test:customBenchmarks'

        # Consolidate output files
        - task: CopyFiles@2
          displayName: Consolidate output files - ${{ testPackage }}
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}/node_modules/${{ testPackage }}/.customBenchmarksOutput
            contents: '**'
            targetFolder: ${{ variables.consolidatedTestsOutputFolder }}/${{ testPackage }}

        # Cleanup package
        - task: Bash@3
          displayName: Cleanup package - ${{ testPackage }}
          inputs:
            targetType: 'inline'
            workingDirectory: ${{ variables.testWorkspace }}/node_modules/
            script: |
              set -eu -o pipefail
              echo "Cleanup package ${{ testPackage }} from ${{ variables.testWorkspace }}/node_modules/"
              rm -rf ${{ testPackage }};

      - task: Bash@3
        displayName: Write measurements to Aria/Kusto
        inputs:
          targetType: 'inline'
          workingDirectory: $(pathToTelemetryGenerator)
          script: |
            set -eu -o pipefail
            echo "Write the following benchmark output to Aria/Kusto";
            ls -laR ${{ variables.consolidatedTestsOutputFolder }};
            npx telemetry-generator --handlerModule $(pathToTelemetryGeneratorHandlers)/customBenchmarkHandler.js --dir ${{ variables.consolidatedTestsOutputFolder }};

      - task: Bash@3
        displayName: Send Custom Data Performance Benchmark Measurments to Azure App Insights
        inputs:
          targetType: 'inline'
          workingDirectory: $(pathToTelemetryGenerator)
          script: |
            set -eu -o pipefail
            echo "Writing performance benchmark output to Azure App Insights..."
            ls -laR ${{ variables.consolidatedTestsOutputFolder }};
            npx telemetry-generator appInsights --handlerModule $(pathToTelemetryGeneratorHandlers)/appInsightsCustomBenchmarkHandler.js --dir '${{ variables.consolidatedTestsOutputFolder }}' --connectionString '$(fluid-interal-app-insights-connection-string)';
        env:
          BUILD_ID: $(Build.BuildId)
          BRANCH_NAME: $(Build.SourceBranchName)

      - task: PublishPipelineArtifact@1
        displayName: Publish Artifact - Perf tests output - custom data usage
        inputs:
          targetPath: '${{ variables.consolidatedTestsOutputFolder }}'
          artifactName: 'perf-test-outputs_custom-data'
        condition: succeededOrFailed()

  - template: /tools/pipelines/templates/include-upload-stage-telemetry.yml@self
    parameters:
      stageId: perf_unit_tests_customBenchmark
      pipelineIdentifierForTelemetry: ${{ variables.pipelineIdentifierForTelemetry }}
      testWorkspace: ${{ variables.testWorkspace }}

  - ${{ each endpointObject in parameters.endpoints }}:

    - stage: perf_e2e_tests_${{ endpointObject.endpointName }}
      displayName: Perf e2e tests - ${{ endpointObject.endpointName }}
      dependsOn: []
      variables:
        # Use contention-lock variable groups when appropriate. Note that null gets coalesced into an empty string.
        - ${{ if ne(endpointObject.lockVariableGroupName, '') }}:
          - group: ${{ endpointObject.lockVariableGroupName }}

        # The following two paths are defined by the npm scripts invocations in @fluid-private/test-end-to-end-tests
        - name: executionTimeTestOutputFolder
          value: ${{ variables.testWorkspace }}/node_modules/@fluid-private/test-end-to-end-tests/.timeTestsOutput
          readonly: true
        - name: memoryUsageTestOutputFolder
          value: ${{ variables.testWorkspace }}/node_modules/@fluid-private/test-end-to-end-tests/.memoryTestsOutput
          readonly: true
        - name: testPackage
          value: '@fluid-private/test-end-to-end-tests'
          readonly: true
      jobs:
      - job: perf_e2e_tests
        displayName: Perf e2e tests
        pool: ${{ parameters.poolBuild }}
        timeoutInMinutes: ${{ coalesce(endpointObject.timeoutInMinutes, 60) }}
        steps:
        - template: templates/include-test-perf-benchmarks.yml
          parameters:
            # ado-feeds-dev and ado-feeds-office come from the ado-feeds variable group
            artifactBuildId: $(artifactBuildId)
            artifactPipeline: $(artifactPipeline)
            devFeedUrl: $(ado-feeds-dev)
            officeFeedUrl: $(ado-feeds-office)
            testFilesPath: $(testFilesPath)
            testWorkspace: $(testWorkspace)

        # Download and install package with performance tests
        - template: templates/include-test-perf-benchmarks-install-package.yml
          parameters:
            artifactBuildId: $(artifactBuildId)
            artifactPipeline: $(artifactPipeline)
            testPackageName: $(testPackage)
            installPath: $(testWorkspace)

        - task: CopyFiles@2
          displayName: Copy .npmrc to test package folder
          inputs:
            sourceFolder: ${{ variables.testWorkspace }}
            contents: '.npmrc'
            targetFolder: ${{ variables.testWorkspace }}/node_modules/${{ variables.testPackage }}
        - task: Npm@1
          displayName: Install dependencies - ${{ endpointObject.endpointName }}
          retryCountOnTaskFailure: 1
          inputs:
            command: 'install'
            workingDir: ${{ variables.testWorkspace }}/node_modules/${{ variables.testPackage }}

        # We run both types of tests in the same bash step so we can make sure to run the second set even if the first
        # one fails.
        # Doing this with separate ADO steps is not easy.
        # This step reports failure if either of the test runs reports failure.
        - task: Bash@3
          displayName: Run tests ${{ endpointObject.endpointName }}
          inputs:
            targetType: 'inline'
            workingDirectory: ${{ variables.testWorkspace }}/node_modules/${{ variables.testPackage }}
            script: |
              # Note: we explicitly do not use 'set -eu -o pipefail' here because we want to run both sets of tests
              # no matter what. Probably should be refactored later to use separate steps for each set of tests.

              echo "FLUID_LOGGER_PROPS = $FLUID_LOGGER_PROPS"

              # Run execution time tests
              echo "FLUID_ENDPOINTNAME = $FLUID_ENDPOINTNAME"
              if [[ '${{ endpointObject.endpointName }}' == 'local' ]]; then
                npm run test:benchmark:report;
              else
                npm run test:benchmark:report:${{ endpointObject.endpointName }};
              fi
              executionTimeTestsExitCode=$?;

              echo "FLUID_ENDPOINTNAME = $FLUID_ENDPOINTNAME"
              # Run memory tests
              if [[ '${{ endpointObject.endpointName }}' == 'local' ]]; then
                npm run test:memory-profiling:report;
              else
                npm run test:memory-profiling:report:${{ endpointObject.endpointName }};
              fi

              memoryTestsExitCode=$?;

              if [[ $executionTimeTestsExitCode -ne 0 ]]; then
                echo "##vso[task.logissue type=error]Exit code for runtime tests execution = $executionTimeTestsExitCode ${{ endpointObject.endpointName }}"
              fi

              if [[ $memoryTestsExitCode -ne 0 ]]; then
                echo "##vso[task.logissue type=error]Exit code for memory tests execution = $memoryTestsExitCode ${{ endpointObject.endpointName }}"
              fi

              if [[ $executionTimeTestsExitCode -ne 0 ]] || [[ $memoryTestsExitCode -ne 0 ]]; then
                exit 1;
              fi
          env:
            FLUID_TEST_LOGGER_PKG_PATH: ${{ variables.testWorkspace }}/node_modules/@ff-internal/aria-logger # Contains getTestLogger impl to inject
            FLUID_BUILD_ID: $(Build.BuildId)
            FLUID_ENDPOINTNAME: ${{ endpointObject.endpointName }}
            FLUID_LOGGER_PROPS: '{ "hostName": "Benchmark", "displayName": "${{variables.pipelineIdentifierForTelemetry}}" }'
            login__microsoft__clientId: $(login-microsoft-clientId)
            ${{ if eq( endpointObject.endpointName, 'odsp' ) }}:
              login__odsp__test__tenants: $(automation-perf-login-odsp-test-tenants)
            ${{ if eq( endpointObject.endpointName, 'frs' ) }}:
              fluid__test__driver__frs: $(automation-fluid-test-driver-frs-perf-test)

        - task: Bash@3
          displayName: Write measurements to Aria/Kusto - execution time ${{ endpointObject.endpointName }}
          condition: succeededOrFailed()
          inputs:
            targetType: 'inline'
            workingDirectory: $(pathToTelemetryGenerator)
            script: |
              set -eu -o pipefail
              echo "Writing the following performance tests results to Aria/Kusto - ${{ endpointObject.endpointName }}"
              ls -la ${{ variables.executionTimeTestOutputFolder }};
              npx telemetry-generator --handlerModule $(pathToTelemetryGeneratorHandlers)/executionTimeTestHandler.js --dir ${{ variables.executionTimeTestOutputFolder }};
          env:
            FLUID_ENDPOINTNAME: ${{ endpointObject.endpointName }}

        - task: Bash@3
          displayName: Write measurements to Aria/Kusto - memory usage ${{ endpointObject.endpointName }}
          condition: succeededOrFailed()
          inputs:
            targetType: 'inline'
            workingDirectory: $(pathToTelemetryGenerator)
            script: |
              set -eu -o pipefail
              echo "Writing the following performance tests results to Aria/Kusto - ${{ endpointObject.endpointName }}"
              ls -la ${{ variables.memoryUsageTestOutputFolder }};
              npx telemetry-generator --handlerModule $(pathToTelemetryGeneratorHandlers)/memoryUsageTestHandler.js --dir ${{ variables.memoryUsageTestOutputFolder }};
          env:
            FLUID_ENDPOINTNAME: ${{ endpointObject.endpointName }}

        - task: Bash@3
          displayName: Send Execution Time Perf Benchmark Measurements to Azure App Insights
          condition: eq('${{ endpointObject.endpointName }}', 'local')
          inputs:
            targetType: 'inline'
            workingDirectory: $(pathToTelemetryGenerator)
            script: |
              set -eu -o pipefail
              echo "Writing execution time performance benchmark output to Azure App Insights..."
              ls -laR ${{ variables.executionTimeTestOutputFolder }};
              npx telemetry-generator appInsights --handlerModule $(pathToTelemetryGeneratorHandlers)/appInsightsExecutionTimeTestHandler.js --dir '${{ variables.executionTimeTestOutputFolder }}' --connectionString '$(fluid-interal-app-insights-connection-string)';
          env:
            BUILD_ID: $(Build.BuildId)
            BRANCH_NAME: $(Build.SourceBranchName)
            FLUID_ENDPOINTNAME: ${{ endpointObject.endpointName }}

        - task: Bash@3
          displayName: Send Memory Usage Perf Benchmark Measurements to Azure App Insights
          condition: eq('${{ endpointObject.endpointName }}', 'local')
          inputs:
            targetType: 'inline'
            workingDirectory: $(pathToTelemetryGenerator)
            script: |
              set -eu -o pipefail
              echo "Writing memory usage performance benchmark output to Azure App Insights..."
              ls -laR ${{ variables.memoryUsageTestOutputFolder }};
              npx telemetry-generator appInsights --handlerModule $(pathToTelemetryGeneratorHandlers)/appInsightsMemoryUsageTestHandler.js --dir '${{ variables.memoryUsageTestOutputFolder }}' --connectionString '$(fluid-interal-app-insights-connection-string)';
          env:
            BUILD_ID: $(Build.BuildId)
            BRANCH_NAME: $(Build.SourceBranchName)
            FLUID_ENDPOINTNAME: ${{ endpointObject.endpointName }}

        - task: PublishPipelineArtifact@1
          displayName: Publish Artifact - E2E perf tests output - execution time {{ $stage }}
          condition: succeededOrFailed()

          inputs:
            targetPath: '${{ variables.executionTimeTestOutputFolder }}'
            artifactName: 'perf-test-outputs-e2e_execution-time-${{ endpointObject.endpointName }}'

        - task: PublishPipelineArtifact@1
          displayName: Publish Artifact - E2E perf tests output - memory usage {{ $stage }}
          condition: succeededOrFailed()
          inputs:
            targetPath: '${{ variables.memoryUsageTestOutputFolder }}'
            artifactName: 'perf-test-outputs-e2e_memory-usage-${{ endpointObject.endpointName }}'

        - task: Bash@3
          displayName: Remove Output Folders from local server run ${{ endpointObject.endpointName }}
          inputs:
            targetType: 'inline'
            workingDirectory: $(pathToTelemetryGenerator)
            script: |
              set -eu -o pipefail
              ls -laR ${{ variables.executionTimeTestOutputFolder }};
              echo "Cleanup  ${{ variables.executionTimeTestOutputFolder }}"
              rm -rf ${{ variables.executionTimeTestOutputFolder }};
              ls -laR ${{ variables.memoryUsageTestOutputFolder }};
              echo "Cleanup  ${{ variables.memoryUsageTestOutputFolder }}"
              rm -rf ${{ variables.memoryUsageTestOutputFolder }};

    - template: /tools/pipelines/templates/include-upload-stage-telemetry.yml@self
      parameters:
        stageId: perf_e2e_tests_${{ endpointObject.endpointName }}
        pipelineIdentifierForTelemetry: ${{ variables.pipelineIdentifierForTelemetry }}
        testWorkspace: ${{ variables.testWorkspace }}
