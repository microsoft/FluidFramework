### Moira service architecture

Accessing an arbitrary state at a commit could be implemented by finding the closest materialized view to the commit, and then traversing the graph – applying the changeSets – to retrieve the state at the desired commit. This becomes expensive if there are many commits that have to be traversed. One possible solution would be to add additional materialized views at intermediate commits. For example, if we keep one materialized view every 100 commits, we can reconstruct every intermediate state with the application of at most 99 changeSets (or 49 changeSets via reversible changeSets by applying the invert operator and traversing backwards from the next materialized view). The disadvantage of this approach is the increase in the required storage space. To keep the access time bounded, we have to insert the intermediate materialized views at fixed distances (either measured in the number of commits or the amount of data in those commits). This means, that in a repository where properties are ingested at a fixed rate, the storage space consumption would grow asymptotically with the square of the ingestion rate. In a repository where we only have modifications, we would have an amortized cost per modification that is no longer constant but linear in the total size of the materialized view.

We need a structure that enables:

 - To store large data-sets in a single repository.
 - Efficiently (in bounded time) retrieve a subset of the state at any commit in the history (random access of data at any point in history).
 - Branching should be supported with zero data copies.
 - The total storage space should not be too high (it should asymptotically be approximately linear in the size of the changeSets).

To achieve this we have designed a data-structure which we call a Materialized History. For a survey of index data-structures that provide random access to arbitrary versions and branching see [1]. Our approach is conceptually similar to the OB+Tree [2], which stores the keyspace as a B-Tree, but reuses shared nodes between different versions. However, we use changeSets to store several versions in one chunk – to reduce the amount of storage needed. This is somewhat similar to a BT-Tree [1], where the leaf nodes also contain all entries for a certain range of keys x versions . In contrast to BT-Trees, we do not store the full version history in one large tree, but have multiple separate roots for each commit.

![Data Structure](../images/moira_data_structure.png)

Illustration of the Materialized History. We store the properties (P1 - P8) in chunks C1 - C5. To access the chunks we use a B-Tree (N1-N5). B-Tree nodes and chunks are shared between commit 1 and commit 2 where possible.

The materialized history divides the changeSet for the materialized view of a given commit into a sequence of chunks of approximately the same size. These chunks are split along the lexicographically sorted absolute paths of the properties in the materialized view. The splitting is done independent from the hierarchy levels in the property tree (e.g. the first chunk might contain all properties with absolute paths between A.A.C to A.B, the next chunk all properties between A.B and D.B.C.E). Splitting the changeSet into chunks in this way makes efficient random access possible. This structure must be optimized for the retrieval of sub-trees that are in the same (or in adjacent) chunks, with a low number of read calls. To determine the chunks that contains the data for a given path, we need to create an additional index. For this, we use a B-Tree, which resolves path ranges to chunk ids.

With the combination of these two data structures, we have a representation, which allows random access into a single materialized view, but does not yet allow us to represent history. We can achieve history, by storing a separate B-Tree for each commit, reusing existing B-Tree nodes (essentially delta encoding the changes to the B-tree). If the corresponding subtree has not been changed, a simple reference to that node suffice. See the figure above for an illustration.

If we would perform a full copy of a chunk every time a modification is applied, we would have to pay a quite high overhead for this representation (chunk size / size of the average modification per chunk). Instead, we can store the chunks via changes – similar to the way we do this for the commits. This creates a ChunkNode which stores the state at the newest chunk for a certain path range, plus a sequence of changeSets from this state backwards that can be used to reconstruct chunks that were used in older B-Trees. This would only be done for a limited number of changes (which is chosen as a certain multiple of the maximum allowed total ChunkNode size). After the allowed size for encoding states via changes has been exceeded, a new ChunkNode is created which contains a full copy of the newest chunk. This way, the retrieval cost for a chunk is always bounded (at most fetching the allowed total ChunkNode size and applying the changes in a node of this size). Similarly, we can also encode the B-Tree nodes via changes, since in most cases only a few entries in a node are going to change. This approach allows us a trade-off between retrieval performance and storage overhead. The longer the sequence of states encoded via changes the more expensive it gets to retrieve a specific historic state, but less storage space is required.

It is important to note, that this type of storage has a different asymptotic performance than storing full copies of the materialized view at intermediate states. Since only the chunks that actually have been modified are duplicated after a certain number of changes, the cost for modifications or insertions does not grow linearly with the total size of the materialized view. For example, if we repeatedly perform a certain number of modifications in one commit, the number of chunks that are affected by those modifications can be at most as high, as the number of modified properties, no matter how big the total repository is. The overhead for the B-Tree is logarithmic in the number of properties in the repository, but we can choose a fairly high base b, so that in most cases it will remain a reasonably small overhead.

#### Literature:
[1] Design and Analysis of Index Structures in MultiVersion Data., Jouini K., Jomier G., New Trends in Data Warehousing and Data Analysis. Annals of Information Systems, vol 3, 2009 Theodoros Tzouramanis, Yannis Manolopoulos, and Nikos A. Lorentzos. Overlapping

[2] Overlapping B+-Trees: An Implementation of a Transaction Time Access Method, Theodoros Tzouramanis, Yannis Manolopoulos, and Nikos A. Lorentzos, Data & Knowledge Engineering, 29(3):381–404, 1999
